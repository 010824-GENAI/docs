{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: More Tokenization, POS Tagging, NER, Word Embeddings, Text Similarity\n",
    "\n",
    "## Agenda\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Part-of-Speech Tagging\n",
    "- Named Entity Recognition\n",
    "- Word Embeddings: Word2Vec\n",
    "- Text Similarity: Cosine Similarity, Jaccard Similarity Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- text processing technique where we reduce words to their root form.\n",
    "- focus on the core meaning of the word instead of being distracted by different ways in which they are being used. \n",
    "- words that they get reduced to may not be dictionary words.\n",
    "- http://snowball.tartarus.org/algorithms/english/stemmer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'artist', 'decid', 'to', 'creat', 'a', 'new', 'paint', '.', 'creatingg', 'creat', 'art', 'is', 'a', 'form', 'of', 'self-express', '.', 'she', 'hope', 'to', 'creat', 'an', 'atmospher', 'of', 'creativ', 'in', 'her', 'studio', 'where', 'she', 'could', 'freeli', 'creat', '.', 'the', 'act', 'of', 'creation', 'brought', 'her', 'joy', ',', 'and', 'she', 'believ', 'that', 'anyon', 'could', 'creat', 'someth', 'beauti', 'with', 'a', 'bit', 'of', 'inspir', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The artist decided to create a new painting. Creating art is a form of self-expression. She hoped to create an atmosphere of creativity in her studio where she could freely create. The act of creation brought her joy, and she believed that anyone could create something beautiful with a bit of inspiration.\"\n",
    "\n",
    "# First, tokenize the words\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# Getting an instance of EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokens]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "- Coming from the word \"lemma\", lemmatizing is finding the lemma of a word. \n",
    "- Lemma in linguistics is the basic form of a word. \n",
    "- Ex: \"be\" would be the lemma for words like \"is\", \"am\", \"are\", \"was\", etc\n",
    "- This technique yields more sophisticated and consistent result than stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can', 'you', 'really', 'have', 'too', 'many', 'pen', '?', 'They', 'all', 'serve', 'different', 'purpose', 'and', 'one', 'simply', 'can', 'not', 'have', 'too', 'many', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "string_for_lemmatizing = \"Can you really have too many pens? They all serve different purposes and one simply cannot have too many!\"\n",
    "tokens = word_tokenize(string_for_lemmatizing)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "- Part of Speech(POS) Tagging refers to a task that identifies each token with their part of speech. \n",
    "- Part of speech is a grammatical concept that denotes which role a word is playing in a sentence. The examples of them would be noun, verb, adverbs, adjectives, pronouns, etc. \n",
    "- enhances the understanding and analysis of textual data. Without POS tagging, we would be limited to the vocabularies and frequencies of appearance in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('run', 'VB'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('going', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('run', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"John's big idea isn't all that bad.\"\n",
    "text = \"I like to run. I am going on a run\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "- Named entities are proper nouns that refer to specific entities. \n",
    "- Named entity recognition is the process of extracting Named Entities from the text.\n",
    "- Closely related to POS tagging and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ner_text = \"\"\"\n",
    "John Doe, a software engineer at ACME Corporation, recently attended a conference in New York City on January 15-17, 2023. The event, organized by Tech Innovations Inc., focused on artificial intelligence and machine learning. During the conference, John had the opportunity to network with professionals from Google, Microsoft, and other leading tech companies.\n",
    "\"\"\"\n",
    "# Tokenize\n",
    "ner_tokens = word_tokenize(ner_text)\n",
    "\n",
    "# Part of Speech Tagging\n",
    "ner_tagged = pos_tag(ner_tokens)\n",
    "\n",
    "# Chunking for Named Entity Recognition\n",
    "result = ne_chunk(ner_tagged)\n",
    "\n",
    "result.draw()\n",
    "\n",
    "# Spacy is great for NER in production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings: Word2Vec, GloVe\n",
    "- Humans are good with words. Computer is good with numbers\n",
    "- Way to represent words as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity\n",
    "- How do we tell if a text is similar to another?\n",
    "- Different matrics such as Euclidean distance, Cosine Similarity, Jaccard Similarity.\n",
    "- Cosine Similarity: Compare the angle of 2 vectors\n",
    "- Jaccard Similarity: Compare how many words they share"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
