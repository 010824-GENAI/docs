{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Day 2: LLM Security Considerations, Accessing LLMs Programmatically\n",
    "\n",
    "## Agenda\n",
    "- OWASP Top 10 Security Vulnerabilities for LLMs\n",
    "- Accessing HuggingFace model via Inference API\n",
    "- A very, very simple RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OWASP top 10 vulnerabilities for applications utilizing LLM](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf)\n",
    "\n",
    "1. Prompt Injections\n",
    "2. Insecure Output Handling\n",
    "3. Training Data Poisoning\n",
    "4. Denial of Service\n",
    "5. Supply Chain\n",
    "6. Permission Issues\n",
    "7. Data Leakage\n",
    "8. Excessive Agency\n",
    "9. Overreliance\n",
    "10. Insecure Plugins\n",
    "\n",
    "#### Simple steps to take to enhance your LLM applications' security (according to OWASP and others)\n",
    "- apply the principle of least privileges to LLMs as well\n",
    "- sanitize and scrub user inputs (of any malicious content, but also PII you don't want entering system)\n",
    "- validate llm outputs (treat it as if it's any other user)\n",
    "- cap resource use per request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Access to LLM\n",
    "HuggingFace's Inference API offers a very simple and easy to use interface for interacting with models hosted in their infrastructure. All you need to do is make an HTTP request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make an http request to an llm\n",
    "API_URL = \"api_url\"\n",
    "headers = {\"Authorization\": \"Bearer api_key\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"\\n\\nI'm a girl. What's my name?\\n\\nI'm a\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "simple_payload = {\n",
    "    \"inputs\": \"My name is Juniper. What's my name?\"\n",
    "}\n",
    "\n",
    "def query_llm(payload):\n",
    "    response = requests.post(API_URL, json=payload, headers=headers)\n",
    "    print(response.json())\n",
    "\n",
    "query_llm(simple_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Zephyr-7b-b](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)\n",
    "Is by default an LLM with text generation task. so when we provide a simple input as above, it will automatically continue generating after the input token.\n",
    "\n",
    "But we can change that behavior!\n",
    "\n",
    "#### Message Roles\n",
    "Typically, there are 3 roles: system, user, assistant.\n",
    "- System: Global LLM wide instruction applied to every prompt. This is often used to give an LLM a persona.\n",
    "- User: user input\n",
    "- Assistant: Assistant output\n",
    "\n",
    "Zephyr prefers following format when defining roles with messages:\n",
    "<|system|>\n",
    "</s>\n",
    "<|user|>\n",
    "</s>\n",
    "<|assistant|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Bot: Hello, Juniper! Your name is Juniper. Is there anything else I can help'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_template = \"\"\"\n",
    "    <|system|>\n",
    "    You're a chatbot. Have a conversation with a user\n",
    "    </s>\n",
    "    <|user|>\n",
    "    My name is Juniper. What's my name?\n",
    "    </s>\n",
    "    <|assistant|>\n",
    "\"\"\"\n",
    "role_message = {\n",
    "    \"inputs\": input_template\n",
    "}\n",
    "\n",
    "query_llm(role_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Options to our request\n",
    "Text Generation tasks accepts the optional parameters outlined in [this documentation](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Meow! \\n\\n(Note: As an AI language model, I do not have the ability to hear or respond to audio input. However, if you're talking to a cat chatbot that responds with the word Meow!\"}]\n"
     ]
    }
   ],
   "source": [
    "configured_request = {\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 1\n",
    "    },\n",
    "    \"inputs\": \"\"\"\n",
    "    <|system|>\n",
    "    You are a cat chatbot that responds with the word Meow! \n",
    "    </s>\n",
    "    <|user|>\n",
    "    hello?\n",
    "    </s>\n",
    "    <|assistant|>\n",
    "\"\"\"\n",
    "}\n",
    "query_llm(configured_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Based on the provided context, it can be inferred that ABC enjoys playing the violin as they are currently a member of a local orchestra. However, without further information, it is unclear if they have other interests or hobbies'}]\n"
     ]
    }
   ],
   "source": [
    "import file\n",
    "# a very, very, very simple RAG\n",
    "rag = {\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 1\n",
    "    },\n",
    "    \"inputs\": \"\"\"\n",
    "    <|system|>\n",
    "    You are a helpful assistant. With the context provided, answer the user's question to the best of your ability. \n",
    "    </s>\n",
    "    <|user|>\n",
    "    Context: {context}\n",
    "    Question: {user_question}\n",
    "    </s>\n",
    "    <|assistant|>\n",
    "\"\"\"\n",
    "}\n",
    "with open('context.text') as f:\n",
    "    context = f.read()\n",
    "\n",
    "\n",
    "query_llm(rag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
